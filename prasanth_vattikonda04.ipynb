{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using several classifiers and tunign parameters - Parameters grid\n",
    "[From official `scikit-learn` documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)\n",
    "\n",
    "\n",
    "Example of usage of the ***model selection*** features of `scikit-learn` and comparison of several classification methods.\n",
    "1. import a sample dataset \n",
    "1. split the dataset into two parts: train and test\n",
    "    - the *train* part will be used for training and validation (i.e. for *development*)\n",
    "    - the *test* part will be used for test (i.e. for *evaluation*)\n",
    "    - the fraction of test data will be _ts_ (a value of your choice between 0.2 and 0.5)\n",
    "1. the function `GridSearchCV` iterates a cross validation experiment to train and test a model with different combinations of paramater values\n",
    "    - for each parameter we set a list of values to test, the function will generate all the combinations\n",
    "    - we choose a *score function* which will be used for the optimization\n",
    "        - e.g. `accuracy_score`, `precision_score`, `cohen_kappa_score`, `f1_score`, see this [page](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for reference\n",
    "    - the output is a dictionary containing \n",
    "        - the set of parameters which maximize the score \n",
    "        - the test scores\n",
    "1. prepare the parameters for the grid\n",
    "    - it is a list of dictionaries\n",
    "1. set the parameters by cross validation and the *score functions* to choose from\n",
    "1. Loop on scores and, for each score, loop on the model labels (see details below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
      "@author: scikit-learn.org and Claudio Sartori\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "@author: scikit-learn.org and Claudio Sartori\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(__doc__) # print information included in the triple quotes at the beginning\n",
    "\n",
    "# Loading a standard dataset\n",
    "dataset = datasets.load_digits()\n",
    "#dataset = datasets.fetch_olivetti_faces()\n",
    "#dataset = datasets.fetch_covtype()\n",
    "#dataset = datasets.load_iris()\n",
    "#dataset = datasets.load_wine()\n",
    "#dataset = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare the environment\n",
    "The `dataset` module contains, among others, a few sample datasets.\n",
    "\n",
    "See this [page](http://scikit-learn.org/stable/datasets/index.html) for reference\n",
    "\n",
    "Prepare the data and the target in X and y. Set `ts`. Set the random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "ts = 0.3\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into the train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1257 examples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=ts, random_state=random_state)\n",
    "print(\"Training on %d examples\" % len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is intended to ease the remainder of the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_lbls = [\n",
    "             'dt', \n",
    "             'nb', \n",
    "             'lp', \n",
    "             'svc', \n",
    "            ]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_param_dt = [{'max_depth': range(1,20)}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          }\n",
    "}\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The function below groups all the outputs\n",
    "Write a function which has as parameter the fitted model and uses the components of the fitted model to inspect the results of the search with the parameters grid.\n",
    "\n",
    "The components are:<br>\n",
    "`model.best_params_`<br>\n",
    "`model.cv_results_['mean_test_score']`<br>`\n",
    "model.cv_results_['std_test_score']`<br>\n",
    "`model.cv_results_['params']`\n",
    "\n",
    "The classification report is generated by the function imported above from sklearn.metrics, which takes as argument the true and the predicted test labels.\n",
    "\n",
    "The +/- in the results is obtained doubling the `std_test_score`\n",
    "\n",
    "The function will be used to print the results for each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "             % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    #print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    #print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loop on scores and, for each score, loop on the model labels\n",
    "- iterate varying the score function\n",
    "    1. iterate varying the classification model among Decision Tree, Naive Bayes, Linear Perceptron, Support Vector\n",
    "        - activate the *grid search*\n",
    "            1. the resulting model will be the best one according to the current score function\n",
    "        - print the best parameter set and the results for each set of parameters using the above defined function\n",
    "        - print the classification report\n",
    "        - store the `.best score_` in a dictionary for a final report\n",
    "    1. print the final report for the current *score funtion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Decision Tree       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'max_depth': 12}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.076 (+/-0.007) for {'max_depth': 1}\n",
      "0.214 (+/-0.027) for {'max_depth': 2}\n",
      "0.422 (+/-0.040) for {'max_depth': 3}\n",
      "0.567 (+/-0.047) for {'max_depth': 4}\n",
      "0.720 (+/-0.036) for {'max_depth': 5}\n",
      "0.769 (+/-0.050) for {'max_depth': 6}\n",
      "0.816 (+/-0.049) for {'max_depth': 7}\n",
      "0.819 (+/-0.045) for {'max_depth': 8}\n",
      "0.827 (+/-0.033) for {'max_depth': 9}\n",
      "0.828 (+/-0.051) for {'max_depth': 10}\n",
      "0.833 (+/-0.035) for {'max_depth': 11}\n",
      "0.843 (+/-0.043) for {'max_depth': 12}\n",
      "0.822 (+/-0.043) for {'max_depth': 13}\n",
      "0.822 (+/-0.043) for {'max_depth': 14}\n",
      "0.840 (+/-0.026) for {'max_depth': 15}\n",
      "0.832 (+/-0.038) for {'max_depth': 16}\n",
      "0.839 (+/-0.032) for {'max_depth': 17}\n",
      "0.826 (+/-0.032) for {'max_depth': 18}\n",
      "0.829 (+/-0.026) for {'max_depth': 19}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        53\n",
      "           1       0.69      0.66      0.67        50\n",
      "           2       0.80      0.74      0.77        47\n",
      "           3       0.75      0.87      0.80        54\n",
      "           4       0.82      0.82      0.82        60\n",
      "           5       0.90      0.86      0.88        66\n",
      "           6       0.85      0.94      0.89        53\n",
      "           7       0.94      0.84      0.88        55\n",
      "           8       0.80      0.81      0.80        43\n",
      "           9       0.77      0.81      0.79        59\n",
      "\n",
      "    accuracy                           0.83       540\n",
      "   macro avg       0.83      0.83      0.83       540\n",
      "weighted avg       0.83      0.83      0.83       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Gaussian Naive Bayes\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'var_smoothing': 0.01}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.896 (+/-0.009) for {'var_smoothing': 10}\n",
      "0.908 (+/-0.034) for {'var_smoothing': 1}\n",
      "0.917 (+/-0.039) for {'var_smoothing': 0.1}\n",
      "0.918 (+/-0.037) for {'var_smoothing': 0.01}\n",
      "0.910 (+/-0.044) for {'var_smoothing': 0.001}\n",
      "0.905 (+/-0.050) for {'var_smoothing': 0.0001}\n",
      "0.902 (+/-0.048) for {'var_smoothing': 1e-05}\n",
      "0.896 (+/-0.046) for {'var_smoothing': 1e-06}\n",
      "0.888 (+/-0.043) for {'var_smoothing': 1e-07}\n",
      "0.878 (+/-0.043) for {'var_smoothing': 1e-08}\n",
      "0.868 (+/-0.039) for {'var_smoothing': 1e-09}\n",
      "0.857 (+/-0.029) for {'var_smoothing': 1e-10}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        53\n",
      "           1       0.90      0.70      0.79        50\n",
      "           2       0.86      0.94      0.90        47\n",
      "           3       0.94      0.85      0.89        54\n",
      "           4       0.95      1.00      0.98        60\n",
      "           5       0.95      0.89      0.92        66\n",
      "           6       0.98      0.96      0.97        53\n",
      "           7       0.93      0.96      0.95        55\n",
      "           8       0.75      0.95      0.84        43\n",
      "           9       0.85      0.86      0.86        59\n",
      "\n",
      "    accuracy                           0.91       540\n",
      "   macro avg       0.91      0.91      0.91       540\n",
      "weighted avg       0.92      0.91      0.91       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Linear Perceptron   \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'early_stopping': True}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.932 (+/-0.019) for {'early_stopping': True}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        53\n",
      "           1       0.60      1.00      0.75        50\n",
      "           2       0.98      0.89      0.93        47\n",
      "           3       1.00      0.96      0.98        54\n",
      "           4       0.97      0.95      0.96        60\n",
      "           5       1.00      0.73      0.84        66\n",
      "           6       0.98      0.85      0.91        53\n",
      "           7       1.00      0.96      0.98        55\n",
      "           8       1.00      0.60      0.75        43\n",
      "           9       0.74      0.93      0.83        59\n",
      "\n",
      "    accuracy                           0.89       540\n",
      "   macro avg       0.92      0.89      0.89       540\n",
      "weighted avg       0.92      0.89      0.89       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Support Vector      \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.989 (+/-0.014) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.017) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.979 (+/-0.006) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.011) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.011) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.969 (+/-0.007) for {'C': 1, 'kernel': 'linear'}\n",
      "0.969 (+/-0.007) for {'C': 10, 'kernel': 'linear'}\n",
      "0.969 (+/-0.007) for {'C': 100, 'kernel': 'linear'}\n",
      "0.969 (+/-0.007) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        53\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       1.00      1.00      1.00        47\n",
      "           3       0.98      0.96      0.97        54\n",
      "           4       1.00      1.00      1.00        60\n",
      "           5       0.99      1.00      0.99        66\n",
      "           6       1.00      1.00      1.00        53\n",
      "           7       0.98      0.98      0.98        55\n",
      "           8       0.98      1.00      0.99        43\n",
      "           9       0.98      0.97      0.97        59\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.99      0.99      0.99       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "\n",
      "Summary of results for precision\n",
      "Estimator\n",
      "Decision Tree       \t - score: 0.84%\n",
      "Gaussian Naive Bayes\t - score: 0.92%\n",
      "Linear Perceptron   \t - score: 0.93%\n",
      "Support Vector      \t - score: 0.99%\n",
      "========================================\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Decision Tree       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'max_depth': 12}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.198 (+/-0.006) for {'max_depth': 1}\n",
      "0.318 (+/-0.015) for {'max_depth': 2}\n",
      "0.465 (+/-0.036) for {'max_depth': 3}\n",
      "0.554 (+/-0.043) for {'max_depth': 4}\n",
      "0.640 (+/-0.024) for {'max_depth': 5}\n",
      "0.739 (+/-0.040) for {'max_depth': 6}\n",
      "0.801 (+/-0.046) for {'max_depth': 7}\n",
      "0.817 (+/-0.020) for {'max_depth': 8}\n",
      "0.824 (+/-0.039) for {'max_depth': 9}\n",
      "0.828 (+/-0.019) for {'max_depth': 10}\n",
      "0.818 (+/-0.038) for {'max_depth': 11}\n",
      "0.831 (+/-0.015) for {'max_depth': 12}\n",
      "0.820 (+/-0.037) for {'max_depth': 13}\n",
      "0.824 (+/-0.038) for {'max_depth': 14}\n",
      "0.819 (+/-0.039) for {'max_depth': 15}\n",
      "0.824 (+/-0.021) for {'max_depth': 16}\n",
      "0.825 (+/-0.017) for {'max_depth': 17}\n",
      "0.822 (+/-0.039) for {'max_depth': 18}\n",
      "0.827 (+/-0.020) for {'max_depth': 19}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94        53\n",
      "           1       0.75      0.78      0.76        50\n",
      "           2       0.86      0.79      0.82        47\n",
      "           3       0.71      0.81      0.76        54\n",
      "           4       0.88      0.83      0.85        60\n",
      "           5       0.88      0.88      0.88        66\n",
      "           6       0.91      0.96      0.94        53\n",
      "           7       0.82      0.85      0.84        55\n",
      "           8       0.79      0.79      0.79        43\n",
      "           9       0.85      0.80      0.82        59\n",
      "\n",
      "    accuracy                           0.84       540\n",
      "   macro avg       0.84      0.84      0.84       540\n",
      "weighted avg       0.85      0.84      0.84       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Gaussian Naive Bayes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on train set:\n",
      "\n",
      "{'var_smoothing': 0.1}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.888 (+/-0.011) for {'var_smoothing': 10}\n",
      "0.905 (+/-0.034) for {'var_smoothing': 1}\n",
      "0.913 (+/-0.038) for {'var_smoothing': 0.1}\n",
      "0.912 (+/-0.040) for {'var_smoothing': 0.01}\n",
      "0.902 (+/-0.048) for {'var_smoothing': 0.001}\n",
      "0.894 (+/-0.059) for {'var_smoothing': 0.0001}\n",
      "0.889 (+/-0.052) for {'var_smoothing': 1e-05}\n",
      "0.881 (+/-0.053) for {'var_smoothing': 1e-06}\n",
      "0.865 (+/-0.059) for {'var_smoothing': 1e-07}\n",
      "0.849 (+/-0.061) for {'var_smoothing': 1e-08}\n",
      "0.831 (+/-0.063) for {'var_smoothing': 1e-09}\n",
      "0.815 (+/-0.061) for {'var_smoothing': 1e-10}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        53\n",
      "           1       0.92      0.72      0.81        50\n",
      "           2       0.88      0.94      0.91        47\n",
      "           3       0.94      0.85      0.89        54\n",
      "           4       0.97      0.98      0.98        60\n",
      "           5       0.97      0.88      0.92        66\n",
      "           6       0.98      1.00      0.99        53\n",
      "           7       0.90      0.98      0.94        55\n",
      "           8       0.75      0.93      0.83        43\n",
      "           9       0.81      0.85      0.83        59\n",
      "\n",
      "    accuracy                           0.91       540\n",
      "   macro avg       0.91      0.91      0.91       540\n",
      "weighted avg       0.92      0.91      0.91       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Linear Perceptron   \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'early_stopping': True}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.929 (+/-0.021) for {'early_stopping': True}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        53\n",
      "           1       0.60      1.00      0.75        50\n",
      "           2       0.98      0.89      0.93        47\n",
      "           3       1.00      0.96      0.98        54\n",
      "           4       0.97      0.95      0.96        60\n",
      "           5       1.00      0.73      0.84        66\n",
      "           6       0.98      0.85      0.91        53\n",
      "           7       1.00      0.96      0.98        55\n",
      "           8       1.00      0.60      0.75        43\n",
      "           9       0.74      0.93      0.83        59\n",
      "\n",
      "    accuracy                           0.89       540\n",
      "   macro avg       0.92      0.89      0.89       540\n",
      "weighted avg       0.92      0.89      0.89       540\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Support Vector      \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on train set:\n",
      "0.989 (+/-0.014) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.966 (+/-0.019) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.979 (+/-0.007) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.011) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.991 (+/-0.014) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.011) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.010) for {'C': 1, 'kernel': 'linear'}\n",
      "0.968 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
      "0.968 (+/-0.010) for {'C': 100, 'kernel': 'linear'}\n",
      "0.968 (+/-0.010) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        53\n",
      "           1       1.00      1.00      1.00        50\n",
      "           2       1.00      1.00      1.00        47\n",
      "           3       0.98      0.96      0.97        54\n",
      "           4       1.00      1.00      1.00        60\n",
      "           5       0.99      1.00      0.99        66\n",
      "           6       1.00      1.00      1.00        53\n",
      "           7       0.98      0.98      0.98        55\n",
      "           8       0.98      1.00      0.99        43\n",
      "           9       0.98      0.97      0.97        59\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.99      0.99      0.99       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "\n",
      "Summary of results for recall\n",
      "Estimator\n",
      "Decision Tree       \t - score: 0.83%\n",
      "Gaussian Naive Bayes\t - score: 0.91%\n",
      "Linear Perceptron   \t - score: 0.93%\n",
      "Support Vector      \t - score: 0.99%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "results_short = {}\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls: \n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        \n",
    "        clf = GridSearchCV(models[m]['estimator'], \n",
    "                           models[m]['param'], \n",
    "                           cv=5,\n",
    "                           scoring='%s_macro' % score,\n",
    "                           iid = False,\n",
    "                           return_train_score = False,\n",
    "                           n_jobs = 2, # this allows using multi-cores\n",
    "                           )\n",
    "        clf.fit(X_train, y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))\n",
    "    print('='*40)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
